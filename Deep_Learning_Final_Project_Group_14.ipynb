{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tjePEdqm7C8T",
        "fhZE-QN57LNV",
        "Dz3CHQGOAraH",
        "2cM-C4x0nE13",
        "CsEpNHFsgwyi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadavLib/DL-Final-Project/blob/main/Deep_Learning_Final_Project_Group_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Installing Required Packages}$"
      ],
      "metadata": {
        "id": "vXrBul1U656S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "96yrwbCjTDoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "EBGkDV2jTYlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "print(wandb.api.api_key is not None)\n"
      ],
      "metadata": {
        "id": "ro2mcdB3TvkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "installing compatible versions of required libraries.\n",
        "We pin transformers to <5.1.0 because as of 2026.01,\n",
        "MoLFormer (ibm/MoLFormer-XL-both-10pct) relies on\n",
        "internal modules such as transformers.onnx and remote\n",
        "configuration code that are not fully compatible with\n",
        "Transformers >=5.1.0. installing torch and onnx ensures\n",
        "model loading and ONNX-related utilities work correctly.\n",
        "'''\n",
        "\n",
        "\n",
        "%pip install -q \\\n",
        "    \"transformers<5.1.0\" \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    wandb \\\n",
        "    peft \\\n",
        "    accelerate \\\n",
        "    umap-learn \\\n",
        "    onnx \\\n",
        "    rdkit"
      ],
      "metadata": {
        "id": "RHIq8JrRRN1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft accelerate datasets evaluate"
      ],
      "metadata": {
        "id": "Lc4U-4UZMTlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "Bhi2oJtemL-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rdkit"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hUdV5Cd9M49C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q umap-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CQhyp-l72QPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem import Draw\n",
        "import pandas as pd\n",
        "\n",
        "#-------------------------------------------------------------------------#\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.special import softmax\n",
        "import evaluate\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "#from peft import LoraConfig, get_peft_model\n",
        "\n",
        "#-------------------------------------------------------------------------#\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "#-------------------------------------------------------------------------#\n",
        "\n",
        "from google.colab import files, drive\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------#\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
        "\n",
        "#-------------------------------------------------------------------------#\n",
        "from datasets import load_dataset\n",
        "#-------------------------------------------------------------------------#\n",
        "from torch.nn.functional import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "#-------------------------------------------------------------------------#\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        ")\n",
        "#-------------------------------------------------------------------------#\n",
        "import shutil\n",
        "#-------------------------------------------------------------------------#\n",
        "import umap"
      ],
      "metadata": {
        "id": "La5jBoLq6g_M",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# 1. Force a \"Thread\" start instead of a \"Spawn\" start.\n",
        "# This often fixes Colab-specific hanging where one notebook is 'cursed'.\n",
        "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
        "\n",
        "# 2. Use your API key directly to bypass the interactive login\n",
        "os.environ[\"WANDB_API_KEY\"] = \"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\"\n",
        "'''"
      ],
      "metadata": {
        "id": "_dVh_jKeqHuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# requires an existing account in wandb. We used a designated API key\n",
        "wandb.login(key=\"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\")\n"
      ],
      "metadata": {
        "id": "nA7Vrsd-evQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# source: https://docs.wandb.ai/models/integrations/huggingface#how-do-i-log-and-view-evaluation-samples-during-training\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"Molformer_DL\" # project name\n",
        "os.environ[\"WANDB_WATCH\"] = \"all\" # logging everything\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "'''"
      ],
      "metadata": {
        "id": "g208VPivB2Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Demonstrating and Validating RDKit Installation}$"
      ],
      "metadata": {
        "id": "tjePEdqm7C8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two different ways to write the SMILES for Aspirin\n",
        "smiles_1 = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
        "smiles_2 = \"O=C(O)c1ccccc1OC(=O)C\"\n",
        "\n",
        "# converting to RDKit Molecule objects\n",
        "mol1 = Chem.MolFromSmiles(smiles_1)\n",
        "mol2 = Chem.MolFromSmiles(smiles_2)\n",
        "\n",
        "# getting the Canonical SMILES\n",
        "can_smiles_1 = Chem.MolToSmiles(mol1)\n",
        "can_smiles_2 = Chem.MolToSmiles(mol2)\n",
        "\n",
        "pd.set_option('display.colheader_justify', 'center')\n",
        "df = pd.DataFrame(columns=['Smiles', 'Canonical Smiles'])\n",
        "df.loc[len(df)] = [smiles_1, can_smiles_1]\n",
        "df.loc[len(df)] = [smiles_2, can_smiles_2]\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "# visualizing them\n",
        "Draw.MolsToGridImage([mol1, mol2], legends=['Version A', 'Version B'])"
      ],
      "metadata": {
        "id": "JbTZQwsa4_MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Motivation through toy example: Visualization of Pooled Embedding vs. First Token Embedding ([CLS], <bos>)}$"
      ],
      "metadata": {
        "id": "1NLNv_aaCQ7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading a common dataset for Chemistry property prediction task (ESol)\n",
        "dataset = load_dataset(\"gayane/esol\")"
      ],
      "metadata": {
        "id": "7cLiAMdxDy5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lMSMkWPeEoT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_df = pd.DataFrame(dataset['train']) # extracting the training molecular sequences\n",
        "\n",
        "'''\n",
        "SMILES (Simplified Molecular Input Line Entry System)\n",
        "is a way to represent chemical structures using a single\n",
        "line of text. The problem is that a single molecule can\n",
        "often be written in many different \"correct\" ways.\n",
        "Canonicalization means to pick exactly one unique version\n",
        "of that string to be the \"standard\" (or \"canonical\") name.\n",
        "Once a SMILES string is canonicalized, it acts like a\n",
        "\"fingerprint\" or a unique ID for that specific molecule.\n",
        "'''\n",
        "\n",
        "toy_df['smiles'] = toy_df['smiles'].apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x), canonical=True, isomericSmiles=True))\n",
        "toy_df"
      ],
      "metadata": {
        "id": "TQHy0BJsGfo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the Molformer foundation model\n",
        "model_name = \"ibm-research/MoLFormer-XL-both-10pct\"\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name, deterministic_eval=True, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "smiles_list = toy_df['smiles'].tolist()\n",
        "\n",
        "# tokenizing the sequences\n",
        "inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "5l2OnrfUDSo3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing tokenization\n",
        "\n",
        "tokenized_input_ids = inputs['input_ids']\n",
        "tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in tokenized_input_ids]\n",
        "\n",
        "for i, toks in enumerate(tokens):\n",
        "    print(f\"{smiles_list[i]} tokens:\\n{toks}\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aUi2l_ILNlWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting inference *token-level* contextual embeddings for toy dataset sequences\n",
        "with torch.no_grad():\n",
        "    out = model(**inputs)"
      ],
      "metadata": {
        "id": "hW06wlTGKMpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verifying that everything worked as expected\n",
        "print(f\"Output size: {out.last_hidden_state.shape}\")\n",
        "print()\n",
        "\n",
        "# printing the desired embeddings\n",
        "print(f\"Output embeddings: {out.last_hidden_state}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QTYEw4P9M-RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating cosine similarity between the pooled output\n",
        "# and the first token embedding in the sequence\n",
        "\n",
        "# Calculating all cosine similarities and storing them\n",
        "cosine_similarities = []\n",
        "pooled_embs = []\n",
        "first_token_embs = []\n",
        "\n",
        "for i in range(len(toy_df)):\n",
        "    pooled_embedding = out.pooler_output[i].unsqueeze(0) # the pooled embedding\n",
        "    first_token_embedding = out.last_hidden_state[i][0].unsqueeze(0) # cls token embedding\n",
        "    pooled_embs.append(pooled_embedding)\n",
        "    first_token_embs.append(first_token_embedding)\n",
        "\n",
        "    similarity = torch.nn.functional.cosine_similarity(pooled_embedding, first_token_embedding)\n",
        "    cosine_similarities.append(similarity.item())\n",
        "\n",
        "# converting to a PyTorch tensor for easier statistical operations\n",
        "cosine_similarities_tensor = torch.tensor(cosine_similarities)\n",
        "\n",
        "# printing descriptive statistics\n",
        "print(f\"\\nDescriptive Statistics for Cosine Similarities:\")\n",
        "print(f\"Average Cosine Similarity: {torch.mean(cosine_similarities_tensor):.4f}\")\n",
        "print(f\"Minimum Cosine Similarity: {torch.min(cosine_similarities_tensor):.4f}\")\n",
        "print(f\"Maximum Cosine Similarity: {torch.max(cosine_similarities_tensor):.4f}\")\n",
        "print(f\"Standard Deviation: {torch.std(cosine_similarities_tensor):.4f}\")\n",
        "print()\n",
        "\n",
        "# plotting a histogram of the cosine similarities\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(cosine_similarities, bins=20, edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribution of Cosine Similarities Between Pooled Output and First Token Embedding')\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBSfRpwhP3Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_embs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f10gjtwBZkHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performing UMAP to compare embeddings of the first token and the pooled one\n",
        "\n",
        "pooled_embs_tensor = torch.cat(pooled_embs, dim=0) # tensor for pooled embeddings\n",
        "first_token_embs_tensor = torch.cat(first_token_embs, dim=0) # tensor for cls token embeddings\n",
        "combined_embeddings = torch.cat((pooled_embs_tensor, first_token_embs_tensor), dim=0) # combined tensor"
      ],
      "metadata": {
        "id": "Nxvn--hFRonV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting tensors to a NumPy array\n",
        "pooled_embs_np = pooled_embs_tensor.cpu().numpy()\n",
        "first_token_embs_np = first_token_embs_tensor.cpu().numpy()\n",
        "combined_embeddings_np = combined_embeddings.cpu().numpy()\n",
        "\n",
        "# Initializing UMAP reducer\n",
        "umap_reducer_2d = umap.UMAP(n_components=2,\n",
        "                            min_dist=0.0,\n",
        "                            metric='cosine',\n",
        "                            random_state=42)\n",
        "\n",
        "# Fitting and transforming the data\n",
        "umap_transformed_embeddings_2d = umap_reducer_2d.fit_transform(combined_embeddings_np)\n",
        "\n",
        "print(f\"Shape of umap_transformed_embeddings: {umap_transformed_embeddings_2d.shape}\")"
      ],
      "metadata": {
        "id": "ziR7fCH5TENW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the transformed embeddings back into pooled and first_token components for plotting\n",
        "half_size = len(toy_df)\n",
        "umap_pooled_output = umap_transformed_embeddings_2d[:half_size]\n",
        "umap_first_token_embeddings = umap_transformed_embeddings_2d[half_size:]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(umap_pooled_output[:, 0], umap_pooled_output[:, 1], label='Pooled Output', alpha=0.7, s=10, color=\"maroon\")\n",
        "plt.scatter(umap_first_token_embeddings[:, 0], umap_first_token_embeddings[:, 1], label='First Token Embeddings', alpha=0.7, s=10)\n",
        "plt.title('UMAP of Pooled Output vs. First Token Embedding')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zPbVHv_ETU8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Setting up required classes for extension approaches}$"
      ],
      "metadata": {
        "id": "fhZE-QN57LNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Classification Head Class in Molformer's repository. Will be used in a custom class.\n",
        "# the attribute \"num_labels\" was added for execution\n",
        "# https://huggingface.co/ibm-research/MoLFormer-XL-both-10pct/blob/main/modeling_molformer.py\n",
        "\n",
        "class MolformerClassificationHead(nn.Module):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dense2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(\n",
        "            config.classifier_dropout_prob\n",
        "            if config.classifier_dropout_prob is not None\n",
        "            else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.classifier_act_fn = ACT2FN[config.hidden_act]\n",
        "        self.skip_connection = config.classifier_skip_connection\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        hidden_state = self.dense(pooled_output)\n",
        "        hidden_state = self.dropout(hidden_state)\n",
        "        hidden_state = self.classifier_act_fn(hidden_state)\n",
        "        if self.skip_connection:\n",
        "            hidden_state = residual = hidden_state + pooled_output\n",
        "        hidden_state = self.dense2(hidden_state)\n",
        "        hidden_state = self.dropout(hidden_state)\n",
        "        hidden_state = self.classifier_act_fn(hidden_state)\n",
        "        if self.skip_connection:\n",
        "            hidden_state = hidden_state + residual\n",
        "        return self.out_proj(hidden_state)"
      ],
      "metadata": {
        "id": "Fv6ce4CUUHOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, label_column=\"label\", task_type=\"cls\"):\n",
        "        processed_data = []\n",
        "\n",
        "        # processing only valid and acceptable smiles sequences,\n",
        "        # otherwise they produce NaN values in predictions\n",
        "        for _, row in df.iterrows():\n",
        "            smiles = self.canonicalize(row['smiles'])\n",
        "            if smiles is not None:\n",
        "                processed_data.append({\n",
        "                    \"smiles\": smiles,\n",
        "                    \"label\": row[label_column]\n",
        "                  })\n",
        "\n",
        "        self.smiles_list = [item['smiles'] for item in processed_data]\n",
        "        self.labels = [item['label'] for item in processed_data]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = 202 # fixed max length that matches the MoLFormer model\n",
        "        self.task_type = task_type # Store task_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    # https://github.com/IBM/molformer/blob/main/notebooks/pretrained_molformer/frozen_embeddings_classification.ipynb\n",
        "    def canonicalize(self, s):\n",
        "        mol = Chem.MolFromSmiles(s) # getting the molecule from smiles sequence\n",
        "        if mol is not None:\n",
        "            return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=True) # after canonicalization - returning to smiles sequence\n",
        "        return None # Returning None if RDKit fails to parse the SMILES\n",
        "\n",
        "\n",
        "    def get_num_classes(self):\n",
        "      # For regression, num_labels is 1. For classification, it's the number of unique labels.\n",
        "      return len(set(self.labels)) if self.task_type == \"cls\" else 1\n",
        "\n",
        "\n",
        "    def get_length_stats(self, mode=\"effective\"):\n",
        "        '''\n",
        "         mode:\n",
        "         'true': tokenized length without truncation/padding\n",
        "         'effective': non-padding tokens after truncation\n",
        "        '''\n",
        "\n",
        "        if not self.smiles_list:\n",
        "            return 0.0, 0, 0\n",
        "\n",
        "        lengths = []\n",
        "\n",
        "        if mode == \"true\":\n",
        "            for smiles in self.smiles_list:\n",
        "                lengths.append(\n",
        "                    len(self.tokenizer(smiles, padding=False, truncation=False)[\"input_ids\"])\n",
        "                )\n",
        "\n",
        "        elif mode == \"effective\":\n",
        "            for i in range(len(self.smiles_list)):\n",
        "              item = self.__getitem__(i)\n",
        "              attention_mask = item['attention_mask']\n",
        "              lengths.append(\n",
        "                    attention_mask.sum().item()\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "        avg = sum(lengths) / len(lengths)\n",
        "        max_length = max(lengths)\n",
        "        min_length = min(lengths)\n",
        "\n",
        "        return avg, max_length, min_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # This converts a SMILES string into a list of numbers (tokens)\n",
        "        smiles = self.smiles_list[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        inputs = self.tokenizer(smiles,\n",
        "                                  truncation=True,\n",
        "                                  padding=\"max_length\",\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "\n",
        "        if self.task_type == \"reg\":\n",
        "            labels = torch.tensor(label, dtype=torch.float)\n",
        "        else:\n",
        "            labels = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels # the column name as required by the hugging face API\n",
        "        }"
      ],
      "metadata": {
        "id": "6-yfNbHq_lGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModelForSequenceClassification(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        # Pass num_labels to the classification head\n",
        "        self.classifier = MolformerClassificationHead(self.encoder.config, num_labels)\n",
        "        self.num_labels = num_labels # Store num_labels\n",
        "\n",
        "        if num_labels == 1:\n",
        "            self.loss_fn = nn.MSELoss()\n",
        "        elif num_labels == 2:\n",
        "            self.loss_fn = nn.CrossEntropyLoss()\n",
        "        else: # For multi-label classification\n",
        "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # take first token in the sequence for all items in the batch\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        logits = self.classifier(cls_embeddings)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1: # Reshape labels for regression to (batch_size, 1)\n",
        "                labels = labels.view(-1, 1).float() # Explicitly convert to float\n",
        "            if self.num_labels > 2: # For multi-label classification, labels might need to be float\n",
        "                labels = labels.float()\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}"
      ],
      "metadata": {
        "id": "u9ZQnbn9_uPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Loading and creating the datasets}$"
      ],
      "metadata": {
        "id": "Dz3CHQGOAraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"ibm/MoLFormer-XL-both-10pct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4Wtck9unNxVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://moleculenet.org/datasets-1\n",
        "\n",
        "def load_datasets(task_type):\n",
        "\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  dataframes = {}\n",
        "  datasets = {}\n",
        "\n",
        "  for filename in uploaded.keys():\n",
        "      name = filename.replace(\".csv\", \"\")  # e.g. bace_train\n",
        "      print(f\"Loaded: {name}\")\n",
        "\n",
        "      # Try reading with 'latin1' encoding to resolve UnicodeDecodeError\n",
        "      try:\n",
        "          df = pd.read_csv(filename)\n",
        "      except UnicodeDecodeError:\n",
        "          df = pd.read_csv(filename, encoding='latin1') # a fallback encoding if the file can't be read (only used for the bbbp files)\n",
        "\n",
        "      dataframes[name] = df\n",
        "      datasets[name] = SMILESDataset(df, tokenizer, task_type=task_type)\n",
        "\n",
        "  return datasets"
      ],
      "metadata": {
        "id": "nEk6OVOj2k0X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_datasets = load_datasets(\"cls\")\n",
        "cls_datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F-k6uq7brmA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_datasets = load_datasets(\"reg\")\n",
        "reg_datasets"
      ],
      "metadata": {
        "id": "SgUT9b8dQ6n9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION TASKS DATASETS\n",
        "\n",
        "bace_train = cls_datasets['bace_train']\n",
        "bace_test = cls_datasets['bace_test']\n",
        "bace_eval = cls_datasets['bace_valid']\n",
        "\n",
        "bbbp_train = cls_datasets['bbbp_train']\n",
        "bbbp_test = cls_datasets['bbbp_test']\n",
        "bbbp_eval = cls_datasets['bbbp_valid']\n",
        "\n",
        "clintox_train = cls_datasets['clintox_train']\n",
        "clintox_test = cls_datasets['clintox_test']\n",
        "clintox_eval = cls_datasets['clintox_valid']\n",
        "\n",
        "'''\n",
        "we wanted to use the hiv dataset, however it was signficantly\n",
        "larger than the rest of the datasets (~33K samples) Therefore,\n",
        "training took longer and we wanted to keep the training datasets\n",
        "as balanced as we can in terms of size.\n",
        "\n",
        "# hiv_train = datasets['hiv_train']\n",
        "# hiv_test = datasets['hiv_test']\n",
        "# hiv_eval = datasets['hiv_valid']\n",
        "'''\n",
        "\n",
        "\n",
        "cls_datasets = {\n",
        "    \"BBBP\": {\"train\": bbbp_train,\n",
        "             \"eval\": bbbp_eval,\n",
        "             \"test\": bbbp_test},\n",
        "\n",
        "    \"BACE\": {\"train\": bace_train,\n",
        "             \"eval\": bace_eval,\n",
        "             \"test\": bace_test},\n",
        "\n",
        "    \"CLINTOX\": {\"train\": clintox_train,\n",
        "                \"eval\": clintox_eval,\n",
        "                \"test\": clintox_test},\n",
        "}\n",
        "\n",
        "'''\n",
        "# ------------------------------------------------------------------------------------------- #\n",
        "\n",
        "# REGRESSION TASKS DATASETS\n",
        "\n",
        "lipo_train = reg_datasets['lipo_train']\n",
        "lipo_test = reg_datasets['lipo_test']\n",
        "lipo_eval = reg_datasets['lipo_valid']\n",
        "\n",
        "esol_train = reg_datasets['esol_train']\n",
        "esol_test = reg_datasets['esol_test']\n",
        "esol_eval = reg_datasets['esol_valid']\n",
        "\n",
        "freesolv_train = reg_datasets['freesolv_train']\n",
        "freesolv_test = reg_datasets['freesolv_test']\n",
        "freesolv_eval = reg_datasets['freesolv_valid']\n",
        "\n",
        "\n",
        "reg_datasets = {\n",
        "    \"LIPO\": {\"train\": lipo_train,\n",
        "             \"eval\": lipo_eval,\n",
        "             \"test\": lipo_test},\n",
        "\n",
        "    \"ESOL\": {\"train\": esol_train,\n",
        "             \"eval\": esol_eval,\n",
        "             \"test\": esol_test},\n",
        "\n",
        "    \"FreeSolv\": {\"train\": freesolv_train,\n",
        "                 \"eval\": freesolv_eval,\n",
        "                 \"test\": freesolv_test},\n",
        "\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "gqEG94xajSZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splits of datasets (train, eval, test) for each classification task\n",
        "splits = {**cls_datasets} #, **reg_datasets}\n",
        "splits"
      ],
      "metadata": {
        "id": "fF-O_GxYjbnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting smiles string statistics:\n",
        "# here we demonstrate the difference between the true length\n",
        "# stats of a SMILES sequence and effective length stats\n",
        "\n",
        "for name, dataset_dict in splits.items():\n",
        "  print(f\"Dataset: {name}\")\n",
        "  print(\"-\" * 50)\n",
        "  eff_avg, eff_max, eff_min = dataset_dict['train'].get_length_stats(mode=\"effective\")\n",
        "  true_avg, true_max, true_min = dataset_dict['train'].get_length_stats(mode=\"true\")\n",
        "\n",
        "  print(f\"Effective Length Stats: \\nAvg: {eff_avg:.2f} \\nMax: {eff_max} \\nMin: {eff_min}\\n\")\n",
        "  print(f\"True Length Stats: \\nAvg: {true_avg:.2f} \\nMax: {true_max} \\nMin: {true_min}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lB1i-npnrOnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Additional auxiliary functions and classes (that rely on the datasets)}$"
      ],
      "metadata": {
        "id": "thq-KXeUmD6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions\n",
        "\n",
        "    # metrics for REGRESSION\n",
        "    if preds.ndim == 1 or preds.shape[-1] == 1:\n",
        "        predictions = preds.reshape(-1)\n",
        "\n",
        "        # Safety check\n",
        "        if np.isnan(predictions).any() or np.isinf(predictions).any():\n",
        "            print(\"WARNING: NaN or Inf detected in regression predictions!\")\n",
        "            predictions = np.nan_to_num(predictions)\n",
        "\n",
        "        mse = mean_squared_error(labels, predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(labels, predictions)\n",
        "        r2 = r2_score(labels, predictions)\n",
        "\n",
        "        return {\n",
        "            \"rmse\": rmse,\n",
        "            \"mse\": mse,\n",
        "            \"mae\": mae,\n",
        "            \"r2\": r2,\n",
        "        }\n",
        "\n",
        "    # metrics for BINARY CLASSIFICATION\n",
        "    elif preds.shape[-1] == 2:\n",
        "        logits = preds\n",
        "\n",
        "        # Safety check\n",
        "        if np.isnan(logits).any() or np.isinf(logits).any():\n",
        "            print(\"WARNING: NaN or Inf detected in logits!\")\n",
        "            logits = np.nan_to_num(logits)\n",
        "\n",
        "        probs = softmax(logits, axis=-1)[:, 1]\n",
        "        preds_cls = (probs >= 0.5).astype(int)\n",
        "\n",
        "        # Safety check\n",
        "        if np.isnan(probs).any() or np.isinf(probs).any():\n",
        "            print(\"WARNING: NaN or Inf detected in probabilities!\")\n",
        "            probs = np.nan_to_num(probs)\n",
        "\n",
        "        return {\n",
        "            \"roc_auc\": roc_auc_score(labels, probs),\n",
        "            \"pr_auc\": average_precision_score(labels, probs),\n",
        "            \"precision\": precision_score(labels, preds_cls, zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds_cls, zero_division=0),\n",
        "            \"f1\": f1_score(labels, preds_cls, zero_division=0),\n",
        "            \"pos_rate\": preds_cls.mean(),\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported prediction shape: {preds.shape}\")\n"
      ],
      "metadata": {
        "id": "LT9x4xVlDaYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints_bace\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "\n",
        "    learning_rate=3e-5,\n",
        "    #weight_decay=0.01,\n",
        "\n",
        "    num_train_epochs=500,  # ‱` see note below\n",
        "    # warmup_ratio=0.1,\n",
        "    # lr_scheduler_type=\"linear\",\n",
        "\n",
        "    dataloader_num_workers=0, # Changed from 8 to 0 to address TypeError\n",
        "\n",
        "    logging_steps=50, # Uncommented to enable logging of training loss\n",
        "    # logging_dir=\"./logs\",\n",
        "\n",
        "    # load_best_model_at_end=True,\n",
        "    # metric_for_best_model=\"loss\",\n",
        "    # greater_is_better=False,\n",
        "\n",
        "    seed=42,\n",
        "    # fp16=True,  # MolFormer typically uses mixed precision\n",
        "    save_safetensors=False, # Disable safetensors to avoid non-contiguous tensor error\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "gwVpFFrSGPFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"roc_auc\",\n",
        "    greater_is_better=True,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=50,\n",
        "    #run_name=\"placeholder\",  # name of the W&B run (optional)\n",
        "    report_to=\"wandb\",  # optional\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "pUcFo4DbETla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://huggingface.co/docs/transformers/v5.0.0rc2/en/main_classes/trainer#transformers.TrainingArguments\n",
        "# source: https://www.manning.com/books/domain-specific-small-language-models\n",
        "\n",
        "def get_training_args(\n",
        "    output_dir,\n",
        "    task_type,  # \"reg\" or \"cls\"\n",
        "    best_metric,\n",
        "    *,\n",
        "    train_batch_size=64,\n",
        "    eval_batch_size=64,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=1e-5, # MolFormer used 3e-5\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=False,\n",
        "    fp16_full_eval=False,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates HuggingFace TrainingArguments with defaults\n",
        "    for molecular property prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Regression - lower is better, Classification - higher is better\n",
        "    greater_is_better = task_type == \"cls\"\n",
        "\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "\n",
        "        metric_for_best_model=best_metric,\n",
        "        greater_is_better=greater_is_better,\n",
        "\n",
        "        per_device_train_batch_size=train_batch_size, # effectively, the desired batch size\n",
        "        per_device_eval_batch_size=eval_batch_size, # effectively, the desired batch size\n",
        "        num_train_epochs=num_train_epochs,\n",
        "\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_ratio=warmup_ratio, # ratio of steps used for a linear warmup from 0 to learning_rate\n",
        "\n",
        "        max_grad_norm=max_grad_norm,   # to stabalize training\n",
        "\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=50,\n",
        "\n",
        "        report_to=\"wandb\",\n",
        "        dataloader_num_workers=0,\n",
        "\n",
        "        fp16=False,  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training\n",
        "        fp16_full_eval=False,  # Whether to use fp16 16-bit (mixed) precision eval instead of 32-bit eval\n",
        "        save_safetensors=False,\n",
        "        seed=seed,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "s0jDBgr7wuWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "model_name = \"ibm/MoLFormer-XL-both-10pct\"\n",
        "\n",
        "# RUNNING FINE-TUNING FOR CLASSIFICATION TASKS\n",
        "def finetune(datasets, task_type, project_name=\"My Project\"):\n",
        "  # task type: \"reg\" - regression or \"cls\" - classification\n",
        "\n",
        "    for task_name, splits in datasets.items():\n",
        "    # task_name corresponds to the property/task that we are fine-tuning for\n",
        "    # splits is the train, eval, and test datasets for each task\n",
        "\n",
        "        # printing display\n",
        "        width = 70\n",
        "        print(\"\\n\" + \"-\" * width)\n",
        "        print(f\" FINE-TUNING TASK: {task_name} \".center(width, \"-\"))\n",
        "        print(f\" TYPE: {task_type.upper()} \".center(width, \"-\"))\n",
        "        print(\"-\" * width)\n",
        "\n",
        "        # creating a folder for each task\n",
        "        task_output_dir = f\"/content/FinetunedModels/{task_name}\"\n",
        "        os.makedirs(task_output_dir, exist_ok=True)\n",
        "\n",
        "        # extracting task-dependent variables\n",
        "        num_labels = splits[\"train\"].get_num_classes()\n",
        "        metric_key = \"roc_auc\" if task_type == \"cls\" else \"rmse\"\n",
        "\n",
        "        if task_type == \"cls\":\n",
        "          metric_key = \"roc_auc\"\n",
        "        elif task_type == \"reg\":\n",
        "          metric_key == \"rmse\"\n",
        "        else:\n",
        "          raise ValueError(\n",
        "              f\"Unknown task type: {task_type}. \"\n",
        "              \"Please use 'cls' for classification or 'reg' for regression.\"\n",
        "          )\n",
        "\n",
        "        # initializing a fresh model per dataset\n",
        "        model = CustomModelForSequenceClassification(model_name, num_labels)\n",
        "\n",
        "        # training args per task\n",
        "        task_training_args = get_training_args(output_dir=task_output_dir,\n",
        "                                               task_type=task_type,\n",
        "                                               best_metric=metric_key)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=task_training_args,\n",
        "            train_dataset=splits[\"train\"],\n",
        "            eval_dataset=splits[\"eval\"],\n",
        "            compute_metrics=compute_metrics,  # custom metric function per task\n",
        "          )\n",
        "\n",
        "        run = wandb.init(\n",
        "            project=\"molformer-finetuning\",\n",
        "            name=f\"finetune-{task_name}\",\n",
        "            reinit=True\n",
        "          )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # evaluating on validation and test set\n",
        "        # a dictionary of metrics per dataset\n",
        "        metrics_val = trainer.evaluate(eval_dataset=splits[\"eval\"])\n",
        "        metrics_test = trainer.evaluate(eval_dataset=splits[\"test\"])\n",
        "\n",
        "        # extracting the specific metric from the dictionary,\n",
        "        # based on the task ('eval_roc_auc' or 'eval_rmse')\n",
        "        eval_key = f\"eval_{metric_key}\"\n",
        "        val_score = metrics_val.get(eval_key, 0.0)\n",
        "        test_score = metrics_test.get(eval_key, 0.0)\n",
        "\n",
        "        # storing in results\n",
        "        results[task_name] = {\n",
        "          f\"val_{metric_key}\": val_score,\n",
        "          f\"test_{metric_key}\": test_score,\n",
        "          \"task_type\": task_type\n",
        "          }\n",
        "\n",
        "        print(f\"[{task_name}] {metric_key.upper()}: VAL - {val_score:.4f} | TEST - {test_score:.4f}\")\n",
        "\n",
        "        shutil.make_archive(f\"/content/{task_name}_model\", 'zip', task_output_dir)\n",
        "\n",
        "        run.finish()\n",
        "\n",
        "    print(\"All results:\", results)"
      ],
      "metadata": {
        "id": "e76ZQ-lhE-hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Finetuning}$"
      ],
      "metadata": {
        "id": "2cM-C4x0nE13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(cls_datasets, \"cls\")"
      ],
      "metadata": {
        "id": "7GOYyCbVdFtw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(reg_datasets, \"reg\")"
      ],
      "metadata": {
        "id": "wddsRvqQdLC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Checking for missing values}$"
      ],
      "metadata": {
        "id": "CsEpNHFsgwyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a4fac2d"
      },
      "source": [
        "'''\n",
        "import numpy as np\n",
        "\n",
        "def check_nan_in_dataset(dataset_name, dataset_split):\n",
        "    print(f\"Checking for NaN values in {dataset_name} - {dataset_split.smiles_list} dataset:\")\n",
        "\n",
        "    # Check smiles_list\n",
        "    smiles_nan = [s for s in dataset_split.smiles_list if s is None or (isinstance(s, float) and np.isnan(s))]\n",
        "    if smiles_nan:\n",
        "        print(f\"  NaN values found in SMILES list: {len(smiles_nan)} instances\")\n",
        "    else:\n",
        "        print(\"  No NaN values found in SMILES list.\")\n",
        "\n",
        "    # Check labels\n",
        "    labels_nan = [l for l in dataset_split.labels if l is None or (isinstance(l, float) and np.isnan(l))]\n",
        "    if labels_nan:\n",
        "        print(f\"  NaN values found in labels: {len(labels_nan)} instances\")\n",
        "    else:\n",
        "        print(\"  No NaN values found in labels.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Check BBBP datasets\n",
        "check_nan_in_dataset(\"BBBP\", datasets_dict[\"BBBP\"][\"train\"])\n",
        "check_nan_in_dataset(\"BBBP\", datasets_dict[\"BBBP\"][\"eval\"])\n",
        "check_nan_in_dataset(\"BBBP\", datasets_dict[\"BBBP\"][\"test\"])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\textbf{Hyperparameter Tuning}$"
      ],
      "metadata": {
        "id": "petrKQEzgaUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from https://wandb.ai/matt24/vit-snacks-sweeps/reports/Hyperparameter-Search-for-HuggingFace-Transformer-Models--VmlldzoyMTUxNTg0\n",
        "\n",
        "'''\n",
        "Weights & Biases Sweeps requires a configuration\n",
        "file to define the hyperparameters to explore,\n",
        "their range of values, the search strategy, etc.\n",
        "'''\n",
        "# randomly searching the different values of hyperparameters\n",
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "}\n",
        "\n",
        "# hyperparameters\n",
        "parameters_dict = {\n",
        "    'epochs': {\n",
        "        'value': 1\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'values': [8, 16, 32, 64]\n",
        "        },\n",
        "    # the learning rate will be sampled log-uniformly\n",
        "    'learning_rate': {\n",
        "        'distribution': 'log_uniform_values',\n",
        "        'min': 1e-5,\n",
        "        'max': 1e-3\n",
        "    },\n",
        "    'weight_decay': {\n",
        "        'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    },\n",
        "}\n",
        "\n",
        "'''\n",
        "we will explore the hyperparameter values for one\n",
        "epoch because later we will fine-tune for more\n",
        "epochs using some of the best combinations of\n",
        "values found with Sweeps\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# adding the parameters dictionary to the configuration\n",
        "sweep_config['parameters'] = parameters_dict\n"
      ],
      "metadata": {
        "id": "RB7B0nElg8Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUGGESTED USING A SWEEP KEY FROM ANOTHER NOTEBOOK"
      ],
      "metadata": {
        "id": "zCySGyuvkBkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "With our Sweep configuration ready, we call wandb.sweep\n",
        "to initialize the hyperparameter search. wandb.sweep\n",
        "takes as input the sweep_config and the project name,\n",
        "and it returns a Sweep ID.\n",
        "'''\n",
        "\n",
        "#import wandb\n",
        "\n",
        "project_name = \"molformer-extension-hyperparameter-tuning\"\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=project_name)\n"
      ],
      "metadata": {
        "id": "HHi-4Wj4iqAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# right now only trying for classification tasks\n",
        "\n",
        "\n",
        "def compute_metrics_fn(eval_preds):\n",
        "\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    precision_metric = evaluate.load(\"precision\")\n",
        "    recall_metric = evaluate.load(\"recall\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    roc_auc_metric = evaluate.load(\"roc_auc\")\n",
        "\n",
        "    logits = eval_preds.predictions\n",
        "    labels = eval_preds.label_ids\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = softmax(logits, axis=-1)[:, 1]\n",
        "\n",
        "    # Predicted class\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    metrics.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
        "    metrics.update(precision_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
        "    metrics.update(recall_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
        "    metrics.update(f1_metric.compute(predictions=preds, references=labels, average='weighted'))\n",
        "\n",
        "    # ROC-AUC (binary case → use probability of positive class)\n",
        "    metrics.update(\n",
        "        roc_auc_metric.compute(\n",
        "        prediction_scores=probs[:, 1],\n",
        "        references=labels\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "9j-bC4y_kK1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BBBP_datasets = splits[\"BBBP\"]\n",
        "BBBP_datasets['train']"
      ],
      "metadata": {
        "id": "cxdkMWDoyxOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config=None):\n",
        "  with wandb.init(config=config):\n",
        "    # set sweep configuration\n",
        "    config = wandb.config\n",
        "\n",
        "    model_name = \"ibm/MoLFormer-XL-both-10pct\"\n",
        "    num_labels = BBBP_datasets[\"train\"].get_num_classes()\n",
        "    model = CustomModelForSequenceClassification(model_name, num_labels)\n",
        "\n",
        "    # set training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='hyperparameter_tuning_pilot_bbbp',\n",
        "\t      report_to='wandb',  # Turn on Weights & Biases logging\n",
        "        num_train_epochs=config.epochs,\n",
        "        learning_rate=config.learning_rate,\n",
        "        weight_decay=config.weight_decay,\n",
        "        per_device_train_batch_size=config.batch_size,\n",
        "        per_device_eval_batch_size=16,\n",
        "        save_strategy='epoch',\n",
        "        eval_strategy='epoch',\n",
        "        logging_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        remove_unused_columns=False,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # define training loop\n",
        "    trainer = Trainer(\n",
        "        # model,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        #data_collator=collate_fn, # happens in a defualt manner\n",
        "        train_dataset=BBBP_datasets['train'],\n",
        "        eval_dataset=BBBP_datasets['eval'],\n",
        "        compute_metrics=compute_metrics_fn\n",
        "    )\n",
        "\n",
        "\n",
        "    # start training loop\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "sQHwgC3_uQcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To actually start the hyperparameter search we call\n",
        "wandb.agent which takes as input the Sweep ID, the\n",
        "train function and the number of experiments we want to run.\n",
        "'''\n",
        "sweep_id = \"u7as4ms5\"\n",
        "wandb.agent(sweep_id, train, count=10)"
      ],
      "metadata": {
        "id": "4tpSYXp8HgKF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

זה מה שיש. תסביר מה לעשות
